name: content-moderation
version: 1.0.0
description: |
  Self-contained content moderation workflow demonstrating AI-like analysis,
  scoring, human review escalation, and publishing - all simulated without external APIs.

inputs:
  content:
    id: "POST-2024-00789"
    type: "user_post"
    authorId: "USER-12345"
    authorTrustScore: 85
    title: "My Amazing Travel Experience"
    body: "Just returned from an incredible trip to Japan. The cherry blossoms were beautiful and the food was amazing! Highly recommend visiting Kyoto and Tokyo."
    mediaUrls:
      - "https://cdn.example.com/images/post-789-1.jpg"
      - "https://cdn.example.com/images/post-789-2.jpg"
    tags: ["travel", "japan", "photography"]
    language: "en"
    submittedAt: "2024-01-15T10:30:00Z"
  authorEmail: "author@example.com"

steps:
  # Step 1: Validate content structure
  - id: validate_content
    activity: validate
    input:
      data: "{{inputs.content}}"
      rules:
        required: ["id", "type", "authorId", "body"]
        types:
          id: "string"
          authorId: "string"
          body: "string"
        custom: "data.body.length > 0 && data.body.length <= 50000"

  # Step 2: Initialize moderation context
  - id: init_moderation
    type: code
    dependsOn: ["validate_content"]
    when: "{{step.validate_content.result.isValid}}"
    input:
      content: "{{inputs.content}}"
    code: |
      const moderationContext = {
        contentId: input.content.id,
        status: 'pending_review',
        startedAt: new Date().toISOString(),
        checks: [],
        flags: [],
        scores: {},
        requiresHumanReview: false,
        autoApprovalEligible: input.content.authorTrustScore >= 90,
        priority: input.content.authorTrustScore < 50 ? 'high' : 'normal'
      };
      
      console.log('Moderation initialized for:', moderationContext.contentId);
      console.log('Auto-approval eligible:', moderationContext.autoApprovalEligible);
      
      return moderationContext;

  # Step 3: Analyze text content (simulated AI)
  - id: ai_text_analysis
    type: code
    dependsOn: ["init_moderation"]
    input:
      content: "{{inputs.content}}"
    code: |
      // Simulated AI text analysis
      const text = (input.content.title + ' ' + input.content.body).toLowerCase();
      
      // Simple keyword-based scoring (simulating AI)
      const toxicWords = ['hate', 'kill', 'attack', 'stupid'];
      const spamPatterns = ['buy now', 'click here', 'free money', 'act fast'];
      
      let toxicityScore = 0;
      let spamScore = 0;
      
      toxicWords.forEach(word => {
        if (text.includes(word)) toxicityScore += 0.3;
      });
      
      spamPatterns.forEach(pattern => {
        if (text.includes(pattern)) spamScore += 0.4;
      });
      
      // Check for excessive caps (shouting)
      const capsRatio = (input.content.body.match(/[A-Z]/g) || []).length / input.content.body.length;
      if (capsRatio > 0.5) toxicityScore += 0.2;
      
      // Check for excessive links
      const linkCount = (text.match(/https?:\/\//g) || []).length;
      if (linkCount > 5) spamScore += 0.3;
      
      console.log('Text analysis complete');
      console.log('Toxicity:', Math.min(toxicityScore, 1));
      console.log('Spam:', Math.min(spamScore, 1));
      
      return {
        results: {
          toxicity: { score: Math.min(toxicityScore, 1), detected: toxicityScore > 0.5 },
          spam: { score: Math.min(spamScore, 1), detected: spamScore > 0.5 },
          hate_speech: { score: 0, detected: false },
          pii_detection: { found: false, types: [] }
        },
        analyzedAt: new Date().toISOString()
      };

  # Step 4: Analyze images (simulated)
  - id: ai_image_analysis
    type: code
    dependsOn: ["init_moderation"]
    when: "{{inputs.content.mediaUrls}}"
    input:
      mediaUrls: "{{inputs.content.mediaUrls}}"
    code: |
      // Simulated image analysis
      const results = input.mediaUrls.map((url, idx) => ({
        imageIndex: idx,
        url,
        adult_content: { score: 0.05, detected: false },
        violence: { score: 0.02, detected: false },
        hate_symbols: { score: 0, detected: false },
        copyright: { detected: false, matches: [] },
        overall_risk: 0.05
      }));
      
      console.log('Analyzed', results.length, 'images');
      
      return {
        results,
        analyzedAt: new Date().toISOString()
      };

  # Step 5: Check spam patterns
  - id: spam_detection
    type: code
    dependsOn: ["init_moderation"]
    input:
      content: "{{inputs.content}}"
    code: |
      // Simulated spam detection
      const body = input.content.body;
      const reasons = [];
      
      // Check for duplicate content patterns
      const words = body.split(/\s+/);
      const uniqueWords = new Set(words);
      if (uniqueWords.size < words.length * 0.5) {
        reasons.push('Repetitive content detected');
      }
      
      // Check for excessive punctuation
      const punctuationRatio = (body.match(/[!?]{2,}/g) || []).length;
      if (punctuationRatio > 3) {
        reasons.push('Excessive punctuation');
      }
      
      const isSpam = reasons.length > 0;
      const confidence = isSpam ? 0.7 : 0.1;
      
      console.log('Spam detection:', isSpam ? 'SPAM' : 'NOT SPAM');
      
      return {
        isSpam,
        confidence,
        reasons,
        checkedAt: new Date().toISOString()
      };

  # Step 6: Aggregate moderation results
  - id: aggregate_results
    type: code
    dependsOn: ["ai_text_analysis", "ai_image_analysis", "spam_detection"]
    input:
      textAnalysis: "{{step.ai_text_analysis.result}}"
      imageAnalysis: "{{step.ai_image_analysis.result}}"
      spamCheck: "{{step.spam_detection.result}}"
      moderationContext: "{{step.init_moderation.result}}"
      authorTrustScore: "{{inputs.content.authorTrustScore}}"
    code: |
      const ctx = input.moderationContext;
      const flags = [];
      const scores = {};
      
      // Process text analysis
      const textResults = input.textAnalysis?.results || {};
      scores.toxicity = textResults.toxicity?.score || 0;
      scores.spam = textResults.spam?.score || 0;
      scores.hateSpeech = textResults.hate_speech?.score || 0;
      
      if (scores.toxicity > 0.7) flags.push({ type: 'toxicity', severity: 'high', score: scores.toxicity });
      if (scores.spam > 0.8) flags.push({ type: 'spam', severity: 'high', score: scores.spam });
      if (scores.hateSpeech > 0.5) flags.push({ type: 'hate_speech', severity: 'critical', score: scores.hateSpeech });
      
      // Process image analysis
      const imageResults = input.imageAnalysis?.results || [];
      scores.imageRisk = Math.max(...imageResults.map(r => r.overall_risk || 0), 0);
      
      // Process spam check
      if (input.spamCheck?.isSpam) {
        flags.push({ type: 'spam_detected', severity: 'high', reasons: input.spamCheck.reasons });
      }
      scores.spamConfidence = input.spamCheck?.confidence || 0;
      
      // Calculate overall risk
      const overallRisk = Math.max(scores.toxicity, scores.spam, scores.hateSpeech, scores.imageRisk, scores.spamConfidence);
      
      // Determine if human review needed
      const criticalFlags = flags.filter(f => f.severity === 'critical');
      const highFlags = flags.filter(f => f.severity === 'high');
      
      const requiresHumanReview = criticalFlags.length > 0 || highFlags.length >= 2 || overallRisk > 0.6 || input.authorTrustScore < 30;
      
      // Determine auto-action
      let autoAction = 'none';
      if (criticalFlags.length > 0 || overallRisk > 0.9) {
        autoAction = 'reject';
      } else if (overallRisk < 0.2 && flags.length === 0 && ctx.autoApprovalEligible) {
        autoAction = 'approve';
      }
      
      console.log('Aggregation complete. Risk:', overallRisk, 'Action:', autoAction);
      
      return {
        ...ctx,
        flags,
        scores,
        overallRisk,
        requiresHumanReview,
        autoAction,
        aggregatedAt: new Date().toISOString()
      };

  # Step 7: Log moderation results
  - id: log_moderation_results
    activity: log
    dependsOn: ["aggregate_results"]
    input:
      message: "Content moderation analysis complete"
      level: "info"
      data:
        contentId: "{{step.aggregate_results.result.contentId}}"
        overallRisk: "{{step.aggregate_results.result.overallRisk}}"
        requiresHumanReview: "{{step.aggregate_results.result.requiresHumanReview}}"
        autoAction: "{{step.aggregate_results.result.autoAction}}"

  # Step 8: Auto-reject if critical violations
  - id: auto_reject
    type: code
    dependsOn: ["aggregate_results"]
    when: "{{step.aggregate_results.result.autoAction}} == 'reject'"
    input:
      results: "{{step.aggregate_results.result}}"
      contentId: "{{inputs.content.id}}"
    code: |
      const rejection = {
        contentId: input.contentId,
        status: 'rejected',
        reason: 'auto_moderation',
        flags: input.results.flags.filter(f => f.severity === 'critical' || f.severity === 'high'),
        rejectedAt: new Date().toISOString(),
        appealEligible: true
      };
      
      console.log('Content auto-rejected:', rejection.contentId);
      
      return rejection;

  # Step 9: Auto-approve if low risk and eligible
  - id: auto_approve
    type: code
    dependsOn: ["aggregate_results"]
    when: "{{step.aggregate_results.result.autoAction}} == 'approve'"
    input:
      results: "{{step.aggregate_results.result}}"
      contentId: "{{inputs.content.id}}"
    code: |
      const approval = {
        contentId: input.contentId,
        status: 'approved',
        reason: 'auto_moderation',
        riskScore: input.results.overallRisk,
        approvedAt: new Date().toISOString(),
        publishable: true
      };
      
      console.log('Content auto-approved:', approval.contentId);
      
      return approval;

  # Step 10: Queue for human review if needed
  - id: queue_human_review
    type: code
    dependsOn: ["aggregate_results"]
    when: "{{step.aggregate_results.result.requiresHumanReview}} == true"
    input:
      results: "{{step.aggregate_results.result}}"
      content: "{{inputs.content}}"
    code: |
      const reviewCase = {
        caseId: 'MOD-' + Date.now(),
        contentId: input.content.id,
        contentType: input.content.type,
        priority: input.results.priority,
        flags: input.results.flags,
        scores: input.results.scores,
        queuedAt: new Date().toISOString()
      };
      
      console.log('Queued for human review:', reviewCase.caseId);
      
      return reviewCase;

  # Step 11: Wait for human review decision
  - id: await_human_review
    type: signal
    dependsOn: ["queue_human_review"]

  # Step 12: Process human review decision
  - id: process_human_decision
    type: code
    dependsOn: ["await_human_review"]
    input:
      decision: "{{step.await_human_review.result}}"
      contentId: "{{inputs.content.id}}"
    code: |
      const decision = input.decision;
      
      const result = {
        contentId: input.contentId,
        status: decision.approved ? 'approved' : 'rejected',
        reviewType: 'human',
        reviewerId: decision.reviewerId,
        reviewedAt: new Date().toISOString(),
        notes: decision.notes || null
      };
      
      if (decision.approved) {
        result.publishable = true;
      } else {
        result.rejectionReason = decision.reason;
        result.appealEligible = true;
      }
      
      console.log('Human review processed:', result.status);
      
      return result;

  # Step 13: Notify author of rejection
  - id: notify_rejection
    activity: email
    dependsOn: ["auto_reject", "process_human_decision"]
    when: "{{step.auto_reject.result.status}} == 'rejected' || {{step.process_human_decision.result.status}} == 'rejected'"
    input:
      to: "{{inputs.authorEmail}}"
      subject: "Your content submission requires attention"
      body: |
        <h1>Content Review Update</h1>
        <p>Your recent submission could not be published as it did not meet our community guidelines.</p>
        
        <h2>Content Details</h2>
        <p><strong>Title:</strong> {{inputs.content.title}}</p>
        <p><strong>Submitted:</strong> {{inputs.content.submittedAt}}</p>
        
        <h2>What You Can Do</h2>
        <p>You may edit your content to address the issues and resubmit, or you can appeal this decision.</p>

  # Step 14: Notify author of approval
  - id: notify_approval
    activity: email
    dependsOn: ["auto_approve", "process_human_decision"]
    when: "{{step.auto_approve.result.publishable}} == true || {{step.process_human_decision.result.publishable}} == true"
    input:
      to: "{{inputs.authorEmail}}"
      subject: "Your content has been published!"
      body: |
        <h1>Content Published!</h1>
        <p>Great news! Your content has been reviewed and published.</p>
        
        <h2>Content Details</h2>
        <p><strong>Title:</strong> {{inputs.content.title}}</p>
        
        <p>Thank you for contributing to our community!</p>

  # Step 15: Log final moderation outcome
  - id: log_final_outcome
    activity: log
    dependsOn: ["notify_rejection", "notify_approval"]
    input:
      message: "Content moderation workflow completed"
      level: "info"
      data:
        contentId: "{{inputs.content.id}}"
        authorId: "{{inputs.content.authorId}}"
